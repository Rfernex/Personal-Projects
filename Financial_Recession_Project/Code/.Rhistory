# Splits the url matrix into two separate matrixes depending on whether scraping is done on a regular url or on a PDF
htm_links <- boj_decision_urls[grepl("\\.htm$", boj_decision_urls$link), , drop = FALSE]
pdf_links <- boj_decision_urls[grepl("\\.pdf$", boj_decision_urls$link), , drop = FALSE]
# Collect the content of each decision statement NO PDF
collect_decision_content <- function(x) {
# Get html code of page
page <- read_html(x)
tibble(
# Keep the URL
url = x,
# Extract text using the entry-content class and all its paragraphs
text = html_elements(page, "#contents p, #contents li") %>%
html_text2(),
# Extract date using the correct class and format
date = html_element(page, ".mod_outer > p:nth-child(1)") %>%
html_text2() %>%
str_extract("[A-Za-z]+ \\d{1,2}, \\d{4}") %>%
mdy()
)
}
# Scrape all statements
decision_content_df <- map(htm_links$link, safely(collect_decision_content), .progress = T) |>
map_df("result")
# Group text by url
grouped_decision_content_df <- decision_content_df %>%
group_by(url) %>%
summarise(all_text = paste(text, collapse = " "))
# Collect the content of each decision statement PDF
# Function to extract date and text separately
extract_boj_content <- function(pdf_url) {
# Create a temporary file
temp_file <- tempfile(fileext = ".pdf")
# Download PDF with proper SSL handling
GET(pdf_url, write_disk(temp_file), config(ssl_verifypeer = FALSE))
# Extract text from PDF
tryCatch({
# Read all pages
pdf_text_content <- pdf_text(temp_file)
# Combine all pages and clean initial whitespace
full_text <- paste(pdf_text_content, collapse = "\n")
full_text <- str_trim(full_text)
# Extract date
date <- str_extract(full_text, "\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}\\b")
# Clean the text:
# 1. Remove multiple spaces
clean_text <- str_replace_all(full_text, "\\s+", " ")
# 2. Remove empty brackets
clean_text <- str_replace_all(clean_text, "\\$$\\s*\\$$", "")
# 3. Clean up newlines and extra spaces
clean_text <- str_replace_all(clean_text, "\\n\\s*\\n", "\n")
# 4. Properly escape dollar signs
clean_text <- str_replace_all(clean_text, "\\$", "")
# Structure the output (only date and text)
result <- list(
url = pdf_url,
date = date,
text = clean_text
)
}, error = function(e) {
result <- list(
url = pdf_url,
date = NA,
text = paste("Error in PDF extraction:", e$message)
)
})
# Clean up
unlink(temp_file)
return(result)
}
# Scrape all statements
decision_content_PDF_df <- map(pdf_links$link, safely(extract_date_and_text), .progress = T) |>
map_df("result")
# Scrape all statements
decision_content_PDF_df <- map(pdf_links$link, safely(extract_boj_content), .progress = T) |>
map_df("result")
# Write CSV files to the specified directory
write.csv(decision_content_PDF_df, file.path(output_dir, "Statements_boj_df.csv"), row.names = FALSE)
View(boj_decision_urls)
View(decision_content_PDF_df)
View(grouped_decision_content_df)
View(grouped_decision_content_df)
write.csv(grouped_decision_content_df, file.path(output_dir, "Statements_boj_tbf_df.csv"), row.names = FALSE)
rm(grouped_decision_content_df)
grouped_decision_content_df <- decision_content_df %>%
group_by(url) %>%
summarise(all_text = paste(text, collapse = " ")) %>% mutate(date = str_extract(all_text, "\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{1,2},\\s\\d{4}\\b"))
View(grouped_decision_content_df)
rm(grouped_decision_content_df)
grouped_decision_content_df <- decision_content_df %>%
group_by(url) %>%
summarise(all_text = paste(text, collapse = " ")) %>% mutate(
date = str_extract(all_text, "\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{1,2},\\s\\d{4}\\b"),
all_text = str_replace(all_text, "^.*\\d{4}\\n", "")
)
View(grouped_decision_content_df)
View(grouped_decision_content_df)
View(grouped_decision_content_df)
rm(grouped_decision_content_df)
# Group text by url
grouped_decision_content_df <- decision_content_df %>%
group_by(url) %>%
summarise(all_text = paste(text, collapse = " ")) %>% mutate(
date = str_extract(all_text, "[A-Za-z]+(\\s|)\\d{1,2}, \\d{4}"),
all_text = str_replace(all_text, "^.*\\d{4}\\n", "")
)
View(grouped_decision_content_df)
rm(grouped_decision_content_df)
decision_content_HTM_df <- decision_content_df %>%
group_by(url) %>%
summarise(all_text = paste(text, collapse = " ")) %>% mutate(
date = str_extract(all_text, "[A-Za-z]+(\\s|)\\d{1,2}, \\d{4}"),
all_text = str_replace(all_text, "^.*\\d{4}\\n", "")
)
View(decision_content_HTM_df)
rm(decision_content_df)
decision_content_HTM_df <- map(htm_links$link, safely(collect_decision_content), .progress = T) |>
map_df("result") %>%
group_by(url) %>%
summarise(all_text = paste(text, collapse = " ")) %>% mutate(
date = str_extract(all_text, "[A-Za-z]+(\\s|)\\d{1,2}, \\d{4}"),
all_text = str_replace(all_text, "^.*\\d{4}\\n", "")
)
View(decision_content_HTM_df)
test <- decision_content_HTM_df %>%  mutate(
date = mdy(date),  # Convert to Date format
month_year = floor_date(date, "month")  # Create month-year column
)
View(test)
rm(test)
test <- decision_content_HTM_df %>%  mutate(
date = mdy(date),  # Convert to Date format
month_year = format(date, "%Y-%m")  # Create month-year as YYYY-MM
)
View(test)
View(decision_content_PDF_df)
View(decision_content_HTM_df)
View(decision_content_PDF_df)
View(decision_content_HTM_df)
rm(test)
decision_content_HTM_df <- map(htm_links$link, safely(collect_decision_content), .progress = T) |>
map_df("result") %>%
group_by(url) %>%
summarise(all_text = paste(text, collapse = " ")) %>% mutate(
date = str_extract(all_text, "[A-Za-z]+(\\s|)\\d{1,2}, \\d{4}"),
all_text = str_replace(all_text, "^.*\\d{4}\\n", "")
) %>%  mutate(
date = mdy(date),  # Convert to Date format
month_year = format(date, "%Y-%m")  # Create month-year as YYYY-MM
)
View(decision_content_PDF_df)
View(decision_content_PDF_df)
View(decision_content_PDF_df)
View(decision_content_HTM_df)
View(decision_content_PDF_df)
View(decision_content_HTM_df)
View(decision_content_HTM_df)
url_boj_decision <- paste0("https://www.boj.or.jp/en/mopo/mpmdeci/state_",1998:2023,"/index.htm")
# Collect the urls of each decision statement
collect_boj_decision_urls <- function(x) {
page <- read_html(x)
urls <- tibble(link = page |>
html_elements(".js-tbl a") |>
html_attr("href")) |>
mutate(link = str_c("https://www.boj.or.jp", link))
return(urls)
}
boj_decision_urls <- map(url_boj_decision, safely(collect_boj_decision_urls), .progress = T) |>
map_df("result")
# Splits the url matrix into two separate matrixes depending on whether scraping is done on a regular url or on a PDF
htm_links <- boj_decision_urls[grepl("\\.htm$", boj_decision_urls$link), , drop = FALSE]
pdf_links <- boj_decision_urls[grepl("\\.pdf$", boj_decision_urls$link), , drop = FALSE]
# Collect the content of each decision statement NO PDF
collect_decision_content <- function(x) {
# Get html code of page
page <- read_html(x)
tibble(
# Keep the URL
url = x,
# Extract text using the entry-content class and all its paragraphs
text = html_elements(page, "#contents p, #contents li") %>%
html_text2(),
# Extract date using the correct class and format
date = html_element(page, ".mod_outer > p:nth-child(1)") %>%
html_text2() %>%
str_extract("[A-Za-z]+ \\d{1,2}, \\d{4}") %>%
mdy()
)
}
# Scrape all statements and format the result table
decision_content_HTM_df <- map(htm_links$link, safely(collect_decision_content), .progress = T) |>
map_df("result") %>%
group_by(url) %>%
summarise(all_text = paste(text, collapse = " ")) %>% mutate(
date = str_extract(all_text, "[A-Za-z]+(\\s|)\\d{1,2}, \\d{4}"),
all_text = str_replace(all_text, "^.*\\d{4}\\n", "")
)
colnames(decision_content_HTM_df)[2] <- "text"
View(decision_content_HTM_df)
extract_boj_content <- function(pdf_url) {
# Create a temporary file
temp_file <- tempfile(fileext = ".pdf")
# Download PDF with proper SSL handling
GET(pdf_url, write_disk(temp_file), config(ssl_verifypeer = FALSE))
# Extract text from PDF
tryCatch({
# Read all pages
pdf_text_content <- pdf_text(temp_file)
# Combine all pages and clean initial whitespace
full_text <- paste(pdf_text_content, collapse = "\n")
full_text <- str_trim(full_text)
# Extract date
date <- str_extract(full_text, "\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}\\b")
# Clean the text:
# 1. Remove multiple spaces
clean_text <- str_replace_all(full_text, "\\s+", " ")
# 2. Remove empty brackets
clean_text <- str_replace_all(clean_text, "\\$$\\s*\\$$", "")
# 3. Clean up newlines and extra spaces
clean_text <- str_replace_all(clean_text, "\\n\\s*\\n", "\n")
# 4. Properly escape dollar signs
clean_text <- str_replace_all(clean_text, "\\$", "")
# Structure the output (only date and text)
result <- list(
url = pdf_url,
date = date,
text = clean_text
)
}, error = function(e) {
result <- list(
url = pdf_url,
date = NA,
text = paste("Error in PDF extraction:", e$message)
)
})
# Clean up
unlink(temp_file)
return(result)
}
# Scrape all statements
decision_content_PDF_df <- map(pdf_links$link, safely(extract_boj_content), .progress = T) |>
map_df("result")
View(decision_content_PDF_df)
decision_content_PDF_df <- map(pdf_links$link, safely(extract_boj_content), .progress = T) |>
map_df("result") %>% mutate(text = str_replace(all_text, "^.*\\d{4}\\n", ""))
View(decision_content_HTM_df)
View(decision_content_PDF_df)
decision_content_PDF_df <- map(pdf_links$link, safely(extract_boj_content), .progress = T) |>
map_df("result") %>% mutate(text = str_replace(text, "^.*\\d{4}\\n", "")) %>% relocate(text, .before = date)
View(decision_content_PDF_df)
decision_content_PDF_df <- map(pdf_links$link, safely(extract_boj_content), .progress = T) |>
map_df("result") %>% mutate(text = str_replace(text, "^.*\\d{4}\\n", "")) %>% relocate(text, .before = date)
View(decision_content_PDF_df)
View(decision_content_PDF_df)
View(decision_content_PDF_df)
rm(decision_content_PDF_df)
decision_content_PDF_df <- map(pdf_links$link, safely(extract_boj_content), .progress = T) |>
map_df("result") %>% mutate(text = str_replace(text, "[A-Za-z]+(\\s|)\\d{1,2},?\\s+\\d{4}", "")) %>% relocate(text, .before = date)
rm(htm_links)
View(decision_content_PDF_df)
View(decision_content_PDF_df)
rm(pdf_links)
decision_content_df <- bind_rows(decision_content_HTM_df, decision_content_PDF_df, ) %>%  mutate(
date = mdy(date),  # Convert to Date format
month_year = format(date, "%Y-%m")  # Create month-year as YYYY-MM
)
View(decision_content_df)
decision_content_df <- bind_rows(decision_content_HTM_df, decision_content_PDF_df, ) %>%  mutate(
date = mdy(date),  # Convert to Date format
month_year = format(date, "%Y-%m")  # Create month-year as YYYY-MM
) %>% arrange(month_year)
View(decision_content_df)
# SCRAPING SURVEY TABLES
# Install and load required packages
if (!requireNamespace("rvest", quietly = TRUE)) {
install.packages("rvest")}
if (!requireNamespace("httr", quietly = TRUE)) {
install.packages("httr")}
if (!requireNamespace("dplyr", quietly = TRUE)) {
install.packages("dplyr")}
library(rvest)
library(httr)
library(dplyr)
# Set the webpage URL
webpage_url <- "https://www.boj.or.jp/en/research/o_survey/index.htm"
output_directory <- "/Users/rfernex/Documents/Education/SciencesPo/Courses/CSS/Project BoJ/Data/pre-processed"  # Specify your directory
# Read the webpage and get the first CSV link
webpage <- read_html(webpage_url)
# Extract all links and get the first CSV one
first_csv_link <- webpage %>%
html_nodes("a") %>%
html_attr("href") %>%
.[grepl("\\.csv$", ., ignore.case = TRUE)] %>%
.[1]
# Make the link absolute if it's relative
if (!grepl("^http", first_csv_link)) {
first_csv_link <- paste0("https://www.boj.or.jp", first_csv_link)
}
# Download the CSV file
file_name <- basename(first_csv_link)
destfile <- file.path(output_directory, file_name)
tryCatch({
GET(first_csv_link, write_disk(destfile, overwrite = TRUE))
message("Downloaded: ", destfile)
}, error = function(e) {
message("Error downloading file: ", e$message)
})
# Import CSV files with Japanese encoding (Shift-JIS)
survey_table <- read_csv(file.path(output_directory, "survey03.csv"),
locale = locale(encoding = "SHIFT-JIS"))
#formats the matrix to remove useless rows and columns
filtered_rows <- survey_table %>%
filter(row_number() %in% c(1,5) | grepl("^\\d{4}", ...1))
filtered_survey_table <- filtered_rows[, c(2,5:30)] %>%   select_if(~!all(is.na(.)))
rm(filtered_rows)
# Create function to extract question dataframe
create_question_df <- function(data, cols) {
# Extract relevant columns including date column
question_df <- data[, c(1, cols)]
# Get the headers from row 2
headers <- as.character(question_df[2, ])
# Replace NA in first column with "Dates"
headers[1] <- "Dates"
# Set the column names
colnames(question_df) <- make.names(headers, unique = TRUE)
# Remove rows 1 and 2 (question and options)
question_df <- question_df[-(1:2), ]
# Keep only rows with years and remove NA rows
question_df <- question_df %>%
filter(grepl("^\\d{4}", Dates)) %>%
filter(!is.na(Dates))
# Remove any columns that are all NA
question_df <- question_df %>%
select_if(~!all(is.na(.)))
# Remove any row that contains NA or are empty
question_df <- question_df %>%
filter_all(any_vars(!is.na(.) & . != "-"))
return(question_df)
}
View(survey_table)
View(webpage)
past_situation_df <- create_question_df(filtered_survey_table, 2:4) %>%
mutate(
month_year = format(parse_date_time(Dates, "Y m!"), "%Y-%m")
) %>%
arrange(month_year)
View(filtered_survey_table)
View(past_situation_df)
rm(past_situation_df)
past_situation_df <- create_question_df(filtered_survey_table, 2:4) %>%
mutate(
month_year = format(parse_date_time(Dates, "Y m!"), "%Y-%m")
) %>%
select(month_year, everything(), -Dates) %>%  # Move month_year first and drop Dates
arrange(month_year)
View(past_situation_df)
current_situation_df <- create_question_df(filtered_survey_table, 12:16) %>%
mutate(
month_year = format(parse_date_time(Dates, "Y m!"), "%Y-%m")
) %>%
select(month_year, everything(), -Dates) %>%  # Move month_year first and drop Dates
arrange(month_year)
future_situation_df <- create_question_df(filtered_survey_table, 17:19)%>%
mutate(
month_year = format(parse_date_time(Dates, "Y m!"), "%Y-%m")
) %>%
select(month_year, everything(), -Dates) %>%  # Move month_year first and drop Dates
arrange(month_year)
perception_rates_df <- create_question_df(filtered_survey_table, 21:22)%>%
mutate(
month_year = format(parse_date_time(Dates, "Y m!"), "%Y-%m")
) %>%
select(month_year, everything(), -Dates) %>%  # Move month_year first and drop Dates
arrange(month_year)
write.csv(past_situation_df, file.path(output_dir, "survey_past_situation_df.csv"), row.names = FALSE)
webpage_url <- "https://www.boj.or.jp/en/research/o_survey/index.htm"
output_directory <- "/Users/rfernex/Documents/Education/SciencesPo/Courses/CSS/Project BoJ/Data/pre-processed"  # Specify your directory
write.csv(past_situation_df, file.path(output_dir, "survey_past_situation_df.csv"), row.names = FALSE)
output_dir <- "/Users/rfernex/Documents/Education/SciencesPo/Courses/CSS/Project BoJ/Data/processed"
write.csv(past_situation_df, file.path(output_dir, "survey_past_situation_df.csv"), row.names = FALSE)
write.csv(current_situation_df, file.path(output_dir, "survey_current_situation_df.csv"), row.names = FALSE)
write.csv(future_situation_df, file.path(output_dir, "survey_future_situation_df.csv"), row.names = FALSE)
write.csv(perception_rates_df, file.path(output_dir, "survey_perception_rates_df.csv"), row.names = FALSE)
# Install and load required packages
if (!require(tidyverse)) install.packages("tidyverse")
if (!require(tidytext)) install.packages("tidytext")
if (!require(rvest)) install.packages("rvest")
if (!require(lubridate)) install.packages("lubridate")
if (!require(purrr)) install.packages("purrr")
if (!require(dplyr)) install.packages("dplyr")
# Load libraries
library(tidyverse)
library(tidytext)
library(rvest)
library(lubridate)
library(purrr)
library(dplyr)
# SCRAPING SPEECHES
# Sets output directory
output_dir <- "/Users/rfernex/Documents/Education/SciencesPo/Courses/CSS/Project BoJ/Data/processed"
# Retrieves main urls
url_boj_speech_after2011 <- paste0("https://www.boj.or.jp/en/about/press/koen_",2011:2023,"/index.htm")
url_boj_speech_before2011 <- paste0("https://www2.boj.or.jp/archive/en/announcements/press/koen_",1998:2010,"/index.htm")
# Single function to collect URLs
collect_boj_speech_urls <- function(x, base_url) {
page <- read_html(x)
tibble(link = page |>
html_elements(".js-tbl a") |>
html_attr("href")) |>
mutate(link = str_c(base_url, link))
}
# Process each set separately
boj_speech_before2011_urls <- map(url_boj_speech_before2011,
~safely(collect_boj_speech_urls)(.x, "https://www2.boj.or.jp"),
.progress = T) %>%
map_df("result")
boj_speech_after2011_urls <- map(url_boj_speech_after2011,
~safely(collect_boj_speech_urls)(.x, "https://www.boj.or.jp"),
.progress = T) %>%
map_df("result")
# Collects the content of each statement AFTER 2011
collect_speech_content_a2011 <- function(x) {
# Get HTML code of the page
page <- read_html(x)
tibble(
# Keep the URL
url = x,
# Extract text using the #contents ID and all its paragraphs
text = html_elements(page, "#contents p, #contents li") %>%
html_text2() %>%
paste(collapse = " "),
# Extract date using the correct class and format
date = html_elements(page, "#contents") %>%
html_text2() %>%
str_extract("(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}") %>%
mdy()
)
}
speech_content_a2011_df <- map(boj_speech_after2011_urls$link, safely(collect_speech_content_a2011), .progress = T) |>
map_df("result")
# Collects the content of each statement BEFORE 2011
htm_links <- boj_speech_before2011_urls[grepl("\\.htm$", boj_speech_before2011_urls$link), , drop = FALSE] #retains only html links (some documents are in pdf)
collect_speech_content_b2011 <- function(x) {
# Get HTML code of the page
page <- read_html(x)
# First get the intro paragraph that contains date and speaker
intro_text <- page %>%
html_elements("body") %>%
html_text2() %>%
str_extract("This article is excerpted and translated from.*?\\d{4}\\.")
tibble(
# Keep the URL
url = x,
# Extract text while excluding submenus (focus on main content only)
text = html_elements(page, "#contents-skip p") %>%
html_text2() %>%
paste(collapse = " "),
# Extract date from intro text
date = html_elements(page, ".main > p") %>%
html_text2() %>%
str_extract("[A-Za-z]+(\\s|)\\d{1,2}, \\d{4}") %>%
.[!is.na(.)] %>%
.[1] %>%  # Take only the first match
mdy()
)
}
speech_content_b2011_df <- map(htm_links$link, safely(collect_speech_content_b2011), .progress = T) |>
map_df("result")
#test <-html_elements(read_html("https://www2.boj.or.jp/archive/en/announcements/press/koen_2010/ko1012e.htm"), ".main > p" ) %>%
#  html_text2() %>%
#  str_extract("[A-Za-z]+(\\s|)\\d{1,2}, \\d{4}") %>%
#  .[!is.na(.)] %>%
#  .[1]
# Concatenates both output tables
speech_content_df <- bind_rows(speech_content_b2011_df, speech_content_a2011_df, )
rm(webpage)
View(speech_content_df)
View(speech_content_df)
speech_content_df <- bind_rows(speech_content_b2011_df, speech_content_a2011_df, ) %>% mutate(
date = mdy(date),  # Convert to Date format
month_year = format(date, "%Y-%m")  # Create month-year as YYYY-MM
) %>%
filter(!is.na(date)) %>%  # Remove rows with NA in the date column
arrange(month_year)
# Concatenates both output tables
speech_content_df <- bind_rows(speech_content_b2011_df, speech_content_a2011_df, )
View(speech_content_df)
speech_content_df <- bind_rows(speech_content_b2011_df, speech_content_a2011_df, ) %>% filter(!is.na(date)) %>%
mutate(
date = mdy(date),  # Convert to Date format
month_year = format(date, "%Y-%m")  # Create month-year as YYYY-MM
) %>%
filter(!is.na(date))
# Concatenates both output tables
speech_content_df <- bind_rows(speech_content_b2011_df, speech_content_a2011_df, )
# Exports output to csv
write.csv(speech_content_df, file.path(output_dir, "speech_boj_df.csv"), row.names = FALSE)
speech_content_df <- bind_rows(speech_content_b2011_df, speech_content_a2011_df) %>%
filter(!is.na(date)) %>%
mutate(
date = parse_date_time(date, "mdy"),  # Use parse_date_time instead of mdy
month_year = format(date, "%Y-%m")  # Create month-year as YYYY-MM
) %>%
filter(!is.na(date))
speech_content_df <- bind_rows(speech_content_b2011_df, speech_content_a2011_df) %>%
filter(!is.na(date)) %>%
mutate(
date = ymd(date),  # Use ymd() instead since dates are in YYYY-MM-DD format
month_year = format(date, "%Y-%m")  # Create month-year as YYYY-MM
) %>%
filter(!is.na(date))
# Exports output to csv
write.csv(speech_content_df, file.path(output_dir, "speech_boj_df.csv"), row.names = FALSE)
